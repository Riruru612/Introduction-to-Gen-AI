{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoyRivSvrqtP"
      },
      "source": [
        "# **Install & Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eEpqNlsri7u",
        "outputId": "145cb919-50e3-4792-bcb8-b389fa0c23cc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs6OAujKr3Fs"
      },
      "source": [
        "# **Load Dataset & Create Paired Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vA35EDGmr4K1"
      },
      "outputs": [],
      "source": [
        "\n",
        "transform_rgb = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                         (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "\n",
        "transform_gray = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,),\n",
        "                         (0.5,))\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Fqcp7SKjr8oW"
      },
      "outputs": [],
      "source": [
        "class PairedCIFAR10(torch.utils.data.Dataset):\n",
        "    def __init__(self, train=True):\n",
        "        self.dataset = torchvision.datasets.CIFAR10(\n",
        "            root='./data',\n",
        "            train=train,\n",
        "            download=True\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, _ = self.dataset[idx]\n",
        "\n",
        "        input_img = transform_gray(img)\n",
        "        target_img = transform_rgb(img)\n",
        "\n",
        "        return input_img, target_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkdr5gB9sHHs",
        "outputId": "560d8388-d66b-4cca-9808-0909d84fc126"
      },
      "outputs": [],
      "source": [
        "train_dataset = PairedCIFAR10(train=True)\n",
        "test_dataset = PairedCIFAR10(train=False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PC6f3MlsKhq"
      },
      "source": [
        "# **Model Definition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tuyDNVTsILN",
        "outputId": "b2d29148-c82d-49a7-cb09-b95352731a9c"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 4, stride=2, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 3, 4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "model = EncoderDecoder().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4jn89ZhsdV0"
      },
      "source": [
        "# **Train Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2oVwIYH4tRTE"
      },
      "outputs": [],
      "source": [
        "def train_model(loss_function, epochs=10):\n",
        "    model = EncoderDecoder().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0002)\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(epoch_loss)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    return model, train_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBCj1GudtZMP"
      },
      "source": [
        "# Train MSE Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMI2WghFtc6y",
        "outputId": "86ce7865-a90f-478b-ac0a-07615e0c6c33"
      },
      "outputs": [],
      "source": [
        "print(\"Training with MSE Loss\")\n",
        "mse_model, mse_losses = train_model(nn.MSELoss(), epochs=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jC8zhQuthHC"
      },
      "source": [
        "# Train L1 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQz8CHN8tiQ9",
        "outputId": "30835132-8fe5-4a87-cf4f-38e42c529671"
      },
      "outputs": [],
      "source": [
        "print(\"Training with L1 Loss\")\n",
        "l1_model, l1_losses = train_model(nn.L1Loss(), epochs=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebkcGUy3slEp"
      },
      "source": [
        "# **Plot Training Loss Comparison**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "YmCGrSRhshcU",
        "outputId": "7ee52806-da1e-4016-83be-fdd91e1bdf21"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(mse_losses, label=\"MSE Loss\")\n",
        "plt.plot(l1_losses, label=\"L1 Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss Comparison\")\n",
        "plt.legend()\n",
        "\n",
        "plt.savefig(\"results/loss_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"Loss graph saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U0S6oHNt0hM"
      },
      "source": [
        "# Visual Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "pJVfo6yJ0hSb",
        "outputId": "5a3de2d5-da06-4930-a1cd-a527ba0b5bcd"
      },
      "outputs": [],
      "source": [
        "def calculate_mae(model):\n",
        "    model.eval()\n",
        "    total_mae = 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            mae = nn.functional.l1_loss(outputs, targets)\n",
        "\n",
        "            total_mae += mae.item()\n",
        "            count += 1\n",
        "\n",
        "    return total_mae / count\n",
        "\n",
        "mse_mae = calculate_mae(mse_model)\n",
        "l1_mae = calculate_mae(l1_model)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(['MSE Model', 'L1 Model'], [mse_mae, l1_mae])\n",
        "plt.ylabel(\"MAE\")\n",
        "plt.title(\"MAE Comparison\")\n",
        "\n",
        "plt.savefig(\"results/mae_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "9yrBQnEat0_X",
        "outputId": "4539a088-67df-4724-e4ad-a96241340ffe"
      },
      "outputs": [],
      "source": [
        "\n",
        "def denormalize(img):\n",
        "    return img * 0.5 + 0.5\n",
        "\n",
        "mse_model.eval()\n",
        "l1_model.eval()\n",
        "\n",
        "dataiter = iter(test_loader)\n",
        "inputs, targets = next(dataiter)\n",
        "\n",
        "inputs = inputs.to(device)\n",
        "targets = targets.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    mse_outputs = mse_model(inputs)\n",
        "    l1_outputs = l1_model(inputs)\n",
        "\n",
        "inputs = inputs.cpu()\n",
        "targets = targets.cpu()\n",
        "mse_outputs = mse_outputs.cpu()\n",
        "l1_outputs = l1_outputs.cpu()\n",
        "\n",
        "fig, axs = plt.subplots(4, 6, figsize=(14,8))\n",
        "\n",
        "for i in range(6):\n",
        "\n",
        "    axs[0, i].imshow(inputs[i].squeeze(), cmap='gray')\n",
        "    axs[0, i].set_title(f\"Sample {i+1}\")\n",
        "    axs[0, i].axis(\"off\")\n",
        "\n",
        "\n",
        "    axs[1, i].imshow(np.transpose(denormalize(mse_outputs[i]).numpy(), (1,2,0)))\n",
        "    axs[1, i].axis(\"off\")\n",
        "\n",
        "\n",
        "    axs[2, i].imshow(np.transpose(denormalize(l1_outputs[i]).numpy(), (1,2,0)))\n",
        "    axs[2, i].axis(\"off\")\n",
        "\n",
        "\n",
        "    axs[3, i].imshow(np.transpose(denormalize(targets[i]).numpy(), (1,2,0)))\n",
        "    axs[3, i].axis(\"off\")\n",
        "\n",
        "axs[0,0].set_ylabel(\"Input\\n(Grayscale)\", fontsize=12)\n",
        "axs[1,0].set_ylabel(\"MSE Output\", fontsize=12)\n",
        "axs[2,0].set_ylabel(\"L1 Output\", fontsize=12)\n",
        "axs[3,0].set_ylabel(\"Ground Truth\", fontsize=12)\n",
        "\n",
        "plt.suptitle(\"Image-to-Image Translation Comparison (MSE vs L1)\", fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.92)\n",
        "\n",
        "plt.savefig(\"results/mse_vs_l1_visual_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"Comparison image saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHOJ_faJt-25"
      },
      "source": [
        "# Save the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVi-xdHut-Yd",
        "outputId": "69dfe224-d00a-415e-b42c-9d9ca2346a40"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "torch.save(mse_model, \"models/mse_full_model.pth\")\n",
        "torch.save(l1_model, \"models/l1_full_model.pth\")\n",
        "\n",
        "print(\"Full models saved successfully!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
